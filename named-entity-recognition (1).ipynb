{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install datasets\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T05:31:44.741380Z","iopub.execute_input":"2024-09-11T05:31:44.741669Z","iopub.status.idle":"2024-09-11T05:31:58.700641Z","shell.execute_reply.started":"2024-09-11T05:31:44.741636Z","shell.execute_reply":"2024-09-11T05:31:58.699526Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the CoNLL-2003 dataset directly from Hugging Face\ndataset = load_dataset(\"conll2003\")\n\n# Check the first few samples of the dataset\nprint(dataset['train'][0])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T10:56:36.182478Z","iopub.execute_input":"2024-09-08T10:56:36.183395Z","iopub.status.idle":"2024-09-08T10:56:37.327476Z","shell.execute_reply.started":"2024-09-08T10:56:36.183353Z","shell.execute_reply":"2024-09-08T10:56:37.326539Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Convert the dataset to a pandas DataFrame\ndf = pd.DataFrame(dataset['train'])\n\n# Print the DataFrame\ndf","metadata":{"execution":{"iopub.status.busy":"2024-09-08T10:49:07.645676Z","iopub.execute_input":"2024-09-08T10:49:07.646399Z","iopub.status.idle":"2024-09-08T10:49:09.480167Z","shell.execute_reply.started":"2024-09-08T10:49:07.646360Z","shell.execute_reply":"2024-09-08T10:49:09.479239Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"          id                                             tokens  \\\n0          0  [EU, rejects, German, call, to, boycott, Briti...   \n1          1                                 [Peter, Blackburn]   \n2          2                             [BRUSSELS, 1996-08-22]   \n3          3  [The, European, Commission, said, on, Thursday...   \n4          4  [Germany, 's, representative, to, the, Europea...   \n...      ...                                                ...   \n14036  14036                                    [on, Friday, :]   \n14037  14037                                    [Division, two]   \n14038  14038                          [Plymouth, 2, Preston, 1]   \n14039  14039                                  [Division, three]   \n14040  14040                           [Swansea, 1, Lincoln, 2]   \n\n                                                pos_tags  \\\n0                    [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n1                                               [22, 22]   \n2                                               [22, 11]   \n3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 3...   \n4      [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 2...   \n...                                                  ...   \n14036                                        [15, 22, 8]   \n14037                                           [21, 11]   \n14038                                   [21, 11, 22, 11]   \n14039                                           [21, 11]   \n14040                                   [21, 11, 22, 11]   \n\n                                              chunk_tags  \\\n0                    [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n1                                               [11, 12]   \n2                                               [11, 12]   \n3      [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 1...   \n4      [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 1...   \n...                                                  ...   \n14036                                        [13, 11, 0]   \n14037                                           [11, 12]   \n14038                                   [11, 12, 12, 12]   \n14039                                           [11, 12]   \n14040                                   [11, 12, 12, 12]   \n\n                                                ner_tags  \n0                            [3, 0, 7, 0, 0, 0, 7, 0, 0]  \n1                                                 [1, 2]  \n2                                                 [5, 0]  \n3      [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...  \n4      [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...  \n...                                                  ...  \n14036                                          [0, 0, 0]  \n14037                                             [0, 0]  \n14038                                       [3, 0, 3, 0]  \n14039                                             [0, 0]  \n14040                                       [3, 0, 3, 0]  \n\n[14041 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tokens</th>\n      <th>pos_tags</th>\n      <th>chunk_tags</th>\n      <th>ner_tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[Peter, Blackburn]</td>\n      <td>[22, 22]</td>\n      <td>[11, 12]</td>\n      <td>[1, 2]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[BRUSSELS, 1996-08-22]</td>\n      <td>[22, 11]</td>\n      <td>[11, 12]</td>\n      <td>[5, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[The, European, Commission, said, on, Thursday...</td>\n      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 3...</td>\n      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 1...</td>\n      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[Germany, 's, representative, to, the, Europea...</td>\n      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 2...</td>\n      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 1...</td>\n      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14036</th>\n      <td>14036</td>\n      <td>[on, Friday, :]</td>\n      <td>[15, 22, 8]</td>\n      <td>[13, 11, 0]</td>\n      <td>[0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>14037</th>\n      <td>14037</td>\n      <td>[Division, two]</td>\n      <td>[21, 11]</td>\n      <td>[11, 12]</td>\n      <td>[0, 0]</td>\n    </tr>\n    <tr>\n      <th>14038</th>\n      <td>14038</td>\n      <td>[Plymouth, 2, Preston, 1]</td>\n      <td>[21, 11, 22, 11]</td>\n      <td>[11, 12, 12, 12]</td>\n      <td>[3, 0, 3, 0]</td>\n    </tr>\n    <tr>\n      <th>14039</th>\n      <td>14039</td>\n      <td>[Division, three]</td>\n      <td>[21, 11]</td>\n      <td>[11, 12]</td>\n      <td>[0, 0]</td>\n    </tr>\n    <tr>\n      <th>14040</th>\n      <td>14040</td>\n      <td>[Swansea, 1, Lincoln, 2]</td>\n      <td>[21, 11, 22, 11]</td>\n      <td>[11, 12, 12, 12]</td>\n      <td>[3, 0, 3, 0]</td>\n    </tr>\n  </tbody>\n</table>\n<p>14041 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Extract words and labels from the dataset\ndef preprocess_data(dataset):\n    words = []\n    labels = []\n    \n    for example in dataset:\n        words.append(example['tokens'])\n        labels.append(example['ner_tags'])\n    \n    return words, labels\n\n# Get train, validation, and test data\ntrain_words, train_labels = preprocess_data(dataset['train'])\nval_words, val_labels = preprocess_data(dataset['validation'])\ntest_words, test_labels = preprocess_data(dataset['test'])\n\n# Create word-to-index dictionary\nfrom collections import Counter\n\n# Create word2idx and label2idx\nword_counts = Counter([word for sentence in train_words for word in sentence])\nword2idx = {word: idx for idx, (word, _) in enumerate(word_counts.items(), start=2)}  # Reserve index 0 for padding, 1 for unknown words\nword2idx[\"<PAD>\"] = 0\nword2idx[\"<UNK>\"] = 1\n\n# Create label2idx using the label names from the dataset\nlabel_names = dataset['train'].features['ner_tags'].feature.names\nlabel2idx = {label: idx for idx, label in enumerate(label_names)}\n\nprint(f\"Word2Idx: {list(word2idx.items())[:5]}\")\nprint(f\"Label2Idx: {label2idx}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T10:56:47.279381Z","iopub.execute_input":"2024-09-08T10:56:47.279992Z","iopub.status.idle":"2024-09-08T10:56:50.058500Z","shell.execute_reply.started":"2024-09-08T10:56:47.279952Z","shell.execute_reply":"2024-09-08T10:56:50.057492Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Word2Idx: [('EU', 2), ('rejects', 3), ('German', 4), ('call', 5), ('to', 6)]\nLabel2Idx: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass BiLSTMNERModel(nn.Module):\n    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n        super(BiLSTMNERModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)  # *2 for bidirectional LSTM\n\n    def forward(self, sentence):\n        embeds = self.embedding(sentence)\n        lstm_out, _ = self.lstm(embeds)\n        tag_space = self.hidden2tag(lstm_out)\n        tag_scores = torch.log_softmax(tag_space, dim=2)\n        return tag_scores\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T10:57:11.540847Z","iopub.execute_input":"2024-09-08T10:57:11.541224Z","iopub.status.idle":"2024-09-08T10:57:13.172093Z","shell.execute_reply.started":"2024-09-08T10:57:11.541189Z","shell.execute_reply":"2024-09-08T10:57:13.171222Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass NERDataset(Dataset):\n    def __init__(self, words, labels, word2idx, label2idx):\n        self.words = words\n        self.labels = labels\n        self.word2idx = word2idx\n        self.label2idx = label2idx\n    \n    def __len__(self):\n        return len(self.words)\n\n    def __getitem__(self, idx):\n        sentence = self.words[idx]\n        label = self.labels[idx]\n        sentence_idx = [self.word2idx.get(word, 1) for word in sentence]  # Convert words to indices, 1 for unknown words\n        label_idx = [self.label2idx[tag] for tag in label]  # Convert labels to indices\n        return torch.tensor(sentence_idx), torch.tensor(label_idx)\n\n# Prepare dataset and dataloader\ndef collate_fn(batch):\n    sentences, labels = zip(*batch)\n    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=0)\n    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)  # Use -100 for padding labels\n    return sentences_padded, labels_padded\n\ntrain_dataset = NERDataset(train_words, train_labels, word2idx, label2idx)\nval_dataset = NERDataset(val_words, val_labels, word2idx, label2idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T10:58:47.727153Z","iopub.execute_input":"2024-09-08T10:58:47.727549Z","iopub.status.idle":"2024-09-08T10:58:47.738615Z","shell.execute_reply.started":"2024-09-08T10:58:47.727509Z","shell.execute_reply":"2024-09-08T10:58:47.737405Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.metrics import classification_report\nimport numpy as np\nfrom datasets import load_dataset\n\n# Check if a GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Step 1: Load the dataset\ndataset = load_dataset(\"conll2003\")\n\n# Step 2: Preprocess data\ndef preprocess_data(dataset, label_names):\n    words = []\n    labels = []\n    \n    for example in dataset:\n        words.append(example['tokens'])\n        labels.append([label_names[tag] for tag in example['ner_tags']])\n    \n    return words, labels\n\n# Get label names from the dataset\nlabel_names = dataset['train'].features['ner_tags'].feature.names\n\n# Get train, validation, and test data\ntrain_words, train_labels = preprocess_data(dataset['train'], label_names)\nval_words, val_labels = preprocess_data(dataset['validation'], label_names)\ntest_words, test_labels = preprocess_data(dataset['test'], label_names)\n\n# Create word2idx and label2idx dictionaries\nfrom collections import Counter\n\n# Create word2idx dictionary\nword_counts = Counter([word for sentence in train_words for word in sentence])\nword2idx = {word: idx for idx, (word, _) in enumerate(word_counts.items(), start=2)}  # Reserve index 0 for padding, 1 for unknown words\nword2idx[\"<PAD>\"] = 0\nword2idx[\"<UNK>\"] = 1\n\n# Create label2idx dictionary\nlabel2idx = {label: idx for idx, label in enumerate(label_names)}\n\nprint(f\"Word2Idx: {list(word2idx.items())[:5]}\")\nprint(f\"Label2Idx: {label2idx}\")\n\n# Step 3: Define NERDataset class\nclass NERDataset(Dataset):\n    def __init__(self, words, labels, word2idx, label2idx):\n        self.words = words\n        self.labels = labels\n        self.word2idx = word2idx\n        self.label2idx = label2idx\n    \n    def __len__(self):\n        return len(self.words)\n\n    def __getitem__(self, idx):\n        sentence = self.words[idx]\n        label = self.labels[idx]\n        sentence_idx = [self.word2idx.get(word, 1) for word in sentence]  # Convert words to indices, 1 for unknown words\n        label_idx = [self.label2idx[tag] for tag in label]  # Convert labels to indices\n        return torch.tensor(sentence_idx), torch.tensor(label_idx)\n\n# Collate function to pad sentences and labels to equal lengths\ndef collate_fn(batch):\n    sentences, labels = zip(*batch)\n    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=0)\n    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)  # Use -100 for padding labels\n    return sentences_padded, labels_padded\n\n# Step 4: Define BiLSTM NER Model\nclass BiLSTMNERModel(nn.Module):\n    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n        super(BiLSTMNERModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)  # *2 for bidirectional LSTM\n\n    def forward(self, sentence):\n        embeds = self.embedding(sentence)\n        lstm_out, _ = self.lstm(embeds)\n        tag_space = self.hidden2tag(lstm_out)\n        tag_scores = torch.log_softmax(tag_space, dim=2)\n        return tag_scores\n\n# Step 5: Define EarlyStopping Class\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0\n\n# Step 6: Define Training and Evaluation Functions\n\n# Function to evaluate the model\ndef evaluate_model(model, val_loader, label2idx):\n    model.eval()\n    true_labels, pred_labels = [], []\n    val_loss = 0\n    loss_function = nn.CrossEntropyLoss(ignore_index=-100)\n    \n    with torch.no_grad():\n        for sentences, labels in val_loader:\n            # Move data to device (GPU/CPU)\n            sentences, labels = sentences.to(device), labels.to(device)\n            \n            tag_scores = model(sentences)\n            predictions = torch.argmax(tag_scores, dim=2)\n\n            # Compute loss for validation set\n            tag_scores = tag_scores.view(-1, len(label2idx))\n            labels = labels.view(-1)\n            loss = loss_function(tag_scores, labels)\n            val_loss += loss.item()\n\n            for i in range(len(sentences)):\n                true_label = [idx2label[l.item()] for l in labels.view(-1)[i*len(sentences[i]):(i+1)*len(sentences[i])] if l.item() != -100]\n                pred_label = [idx2label[p.item()] for p, l in zip(predictions[i], labels.view(-1)[i*len(sentences[i]):(i+1)*len(sentences[i])]) if l.item() != -100]\n                \n                true_labels.extend(true_label)\n                pred_labels.extend(pred_label)\n\n    val_loss /= len(val_loader)  # Average validation loss\n    return val_loss, classification_report(true_labels, pred_labels, output_dict=True)['weighted avg']['f1-score']\n\n# Training function with Early Stopping\ndef train_model(model, train_loader, val_loader, label2idx, num_epochs=100, learning_rate=0.001, patience=5):\n    loss_function = nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding labels\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    early_stopping = EarlyStopping(patience=patience)\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for sentences, labels in train_loader:\n            # Move data to device (GPU/CPU)\n            sentences, labels = sentences.to(device), labels.to(device)\n\n            model.zero_grad()\n            tag_scores = model(sentences)\n            \n            # Flatten the output and labels to compute the loss\n            tag_scores = tag_scores.view(-1, len(label2idx))\n            labels = labels.view(-1)\n\n            loss = loss_function(tag_scores, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_train_loss = total_loss / len(train_loader)\n        val_loss, val_f1 = evaluate_model(model, val_loader, label2idx)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n\n        # Check for early stopping\n        early_stopping(val_loss)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered.\")\n            break\n\n# Step 7: Prepare DataLoaders\ntrain_dataset = NERDataset(train_words, train_labels, word2idx, label2idx)\nval_dataset = NERDataset(val_words, val_labels, word2idx, label2idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n# Step 8: Initialize Model and Train\nvocab_size = len(word2idx)\ntagset_size = len(label2idx)\nidx2label = {idx: label for label, idx in label2idx.items()}\n\nmodel = BiLSTMNERModel(vocab_size, tagset_size).to(device)  # Move the model to the GPU\n\ntrain_model(model, train_loader, val_loader, label2idx)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:07:56.700367Z","iopub.execute_input":"2024-09-08T11:07:56.700767Z","iopub.status.idle":"2024-09-08T11:09:06.962618Z","shell.execute_reply.started":"2024-09-08T11:07:56.700720Z","shell.execute_reply":"2024-09-08T11:09:06.961701Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Using device: cuda\nWord2Idx: [('EU', 2), ('rejects', 3), ('German', 4), ('call', 5), ('to', 6)]\nLabel2Idx: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\nEpoch 1/100, Train Loss: 0.5805, Val Loss: 0.4321, Val F1: 0.8621\nEpoch 2/100, Train Loss: 0.2676, Val Loss: 0.2875, Val F1: 0.9158\nEpoch 3/100, Train Loss: 0.1532, Val Loss: 0.2600, Val F1: 0.9293\nEpoch 4/100, Train Loss: 0.0892, Val Loss: 0.2433, Val F1: 0.9372\nEpoch 5/100, Train Loss: 0.0490, Val Loss: 0.2583, Val F1: 0.9401\nEpoch 6/100, Train Loss: 0.0254, Val Loss: 0.2951, Val F1: 0.9407\nEpoch 7/100, Train Loss: 0.0124, Val Loss: 0.3337, Val F1: 0.9404\nEpoch 8/100, Train Loss: 0.0061, Val Loss: 0.3644, Val F1: 0.9398\nEpoch 9/100, Train Loss: 0.0033, Val Loss: 0.3929, Val F1: 0.9403\nEarly stopping triggered.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model\ndef save_model(model, path=\"ner_model.pth\"):\n    torch.save(model.state_dict(), path)\n    print(f\"Model saved to {path}\")\n\n# Call this after training to save the model\nsave_model(model, \"ner_model.pth\")\n# Load the model\ndef load_model(vocab_size, tagset_size, path=\"ner_model.pth\"):\n    model = BiLSTMNERModel(vocab_size, tagset_size)  # Initialize the model architecture\n    model.load_state_dict(torch.load(path))  # Load the saved weights\n    model.to(device)  # Move model to the correct device (GPU/CPU)\n    model.eval()  # Set the model to evaluation mode\n    print(f\"Model loaded from {path}\")\n    return model\n\n# Example of loading the model\nvocab_size = len(word2idx)\ntagset_size = len(label2idx)\nloaded_model = load_model(vocab_size, tagset_size, \"ner_model.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:12:11.313775Z","iopub.execute_input":"2024-09-08T11:12:11.314189Z","iopub.status.idle":"2024-09-08T11:12:11.386818Z","shell.execute_reply.started":"2024-09-08T11:12:11.314150Z","shell.execute_reply":"2024-09-08T11:12:11.385856Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Model saved to ner_model.pth\nModel loaded from ner_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1578491870.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(path))  # Load the saved weights\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_with_model(model, sentence, word2idx, idx2label):\n    model.eval()  # Set the model to evaluation mode\n    sentence_idx = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in sentence.split()]\n    sentence_tensor = torch.tensor(sentence_idx).unsqueeze(0).to(device)  # Batch of 1 sentence\n\n    with torch.no_grad():\n        tag_scores = model(sentence_tensor)\n        predictions = torch.argmax(tag_scores, dim=2)\n\n    # Convert predictions back to labels\n    predicted_labels = [idx2label[p.item()] for p in predictions[0]]\n    return list(zip(sentence.split(), predicted_labels))\n\n# Example prediction\nsentence = \" i love the fat girl\"\npredictions = predict_with_model(loaded_model, sentence, word2idx, idx2label)\nprint(predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:13:58.636305Z","iopub.execute_input":"2024-09-08T11:13:58.637344Z","iopub.status.idle":"2024-09-08T11:13:58.647297Z","shell.execute_reply.started":"2024-09-08T11:13:58.637285Z","shell.execute_reply":"2024-09-08T11:13:58.646246Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[('i', 'O'), ('love', 'O'), ('the', 'O'), ('fat', 'O'), ('girl', 'O')]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorForTokenClassification\nfrom sklearn.metrics import classification_report\nfrom transformers import EarlyStoppingCallback\nimport numpy as np\n\n# Check if a GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Step 1: Load the CoNLL-2003 dataset\ndataset = load_dataset(\"conll2003\")\n\n# Step 2: Load the BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# Step 3: Preprocess the dataset\nlabel_list = dataset['train'].features['ner_tags'].feature.names  # Get NER label names (e.g., 'B-PER', 'I-LOC', etc.)\nlabel_to_id = {l: i for i, l in enumerate(label_list)}\nid_to_label = {i: l for l, i in label_to_id.items()}\n\n# Tokenize and align labels for BERT input\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples['ner_tags']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)  # Label ignored in loss calculation\n            elif word_idx != previous_word_idx:  # First sub-token of the word\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)  # Ignore sub-tokens\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n# Apply tokenization and label alignment\ntokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n\n# Step 4: Load Pre-trained BERT Model for Token Classification\nmodel = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_list)).to(device)\n\n# Step 5: Data Collator and Training Arguments\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=30,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,  # Adjust this as needed\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True\n)\n\n\n\n# Step 6: Define Metric for Evaluation (F1-score)\nfrom sklearn.metrics import classification_report\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Flatten true labels and predictions for comparison\n    true_labels = [[id_to_label[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [id_to_label[pred] for (pred, lab) in zip(prediction, label) if lab != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    # Flatten the list of lists into a single list\n    true_labels_flat = [item for sublist in true_labels for item in sublist]\n    true_predictions_flat = [item for sublist in true_predictions for item in sublist]\n    \n    # Generate classification report\n    results = classification_report(true_labels_flat, true_predictions_flat, output_dict=True)\n    \n    return {\n        \"precision\": results[\"weighted avg\"][\"precision\"],\n        \"recall\": results[\"weighted avg\"][\"recall\"],\n        \"f1\": results[\"weighted avg\"][\"f1-score\"],\n        \"accuracy\": results[\"accuracy\"]\n    }\n\n# Step 7: Trainer Setup for Fine-Tuning with Early Stopping\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Early stopping if no improvement after 3 evaluations\n)\n\n# Step 8: Fine-tune the model\ntrainer.train()\n\n# Step 9: Save the fine-tuned model\nmodel.save_pretrained(\"bert-ner-model\")\ntokenizer.save_pretrained(\"bert-ner-tokenizer\")\n\n# Step 10: Load the saved model for future use\ndef load_model():\n    model = BertForTokenClassification.from_pretrained(\"bert-ner-model\").to(device)\n    tokenizer = BertTokenizerFast.from_pretrained(\"bert-ner-tokenizer\")\n    return model, tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T07:17:22.553090Z","iopub.execute_input":"2024-09-11T07:17:22.553602Z","iopub.status.idle":"2024-09-11T07:30:48.288033Z","shell.execute_reply.started":"2024-09-11T07:17:22.553553Z","shell.execute_reply":"2024-09-11T07:30:48.286899Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2634' max='13170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2634/13170 13:21 < 53:27, 3.29 it/s, Epoch 6/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.072000</td>\n      <td>0.056684</td>\n      <td>0.984221</td>\n      <td>0.984074</td>\n      <td>0.983968</td>\n      <td>0.984074</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.039200</td>\n      <td>0.047057</td>\n      <td>0.987889</td>\n      <td>0.987831</td>\n      <td>0.987848</td>\n      <td>0.987831</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.024100</td>\n      <td>0.045954</td>\n      <td>0.988938</td>\n      <td>0.989000</td>\n      <td>0.988960</td>\n      <td>0.989000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.015700</td>\n      <td>0.049494</td>\n      <td>0.988827</td>\n      <td>0.988766</td>\n      <td>0.988770</td>\n      <td>0.988766</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.010000</td>\n      <td>0.051928</td>\n      <td>0.988718</td>\n      <td>0.988747</td>\n      <td>0.988729</td>\n      <td>0.988747</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.008500</td>\n      <td>0.057711</td>\n      <td>0.988463</td>\n      <td>0.988416</td>\n      <td>0.988424</td>\n      <td>0.988416</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Function to provide meanings for the NER tags\ndef explain_ner_tag(tag):\n    tag_explanations = {\n        'O': 'Outside of any entity',\n        'B-PER': 'Beginning of a Person entity',\n        'I-PER': 'Inside a Person entity',\n        'B-LOC': 'Beginning of a Location entity',\n        'I-LOC': 'Inside a Location entity',\n        'B-ORG': 'Beginning of an Organization entity',\n        'I-ORG': 'Inside an Organization entity',\n        'B-MISC': 'Beginning of a Miscellaneous entity',\n        'I-MISC': 'Inside a Miscellaneous entity',\n    }\n    return tag_explanations.get(tag, \"Unknown tag\")\n\n# Function to predict NER tags and explain them\ndef predict_with_model(model, tokenizer, sentence):\n    model.eval()\n    tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, is_split_into_words=False).to(device)\n    with torch.no_grad():\n        output = model(**tokens)\n    predictions = torch.argmax(output.logits, dim=2)\n    predicted_labels = [id_to_label[i.item()] for i in predictions[0]]\n\n    # Include explanations with predictions\n    explained_predictions = []\n    for token, tag in zip(tokenizer.tokenize(sentence), predicted_labels):\n        explained_predictions.append((token, tag, explain_ner_tag(tag)))\n    \n    return explained_predictions\n\n# Example usage for prediction and explanation\nsentence = \"i am Md Sazzat Hossain. I love Bangladesh\"\nmodel, tokenizer = load_model()  # Assuming the load_model function is defined\npredictions = predict_with_model(model, tokenizer, sentence)\n\n# Print predictions with explanations\nfor token, tag, explanation in predictions:\n    print(f\"Token: {token}, Tag: {tag}, Meaning: {explanation}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T07:16:02.399577Z","iopub.execute_input":"2024-09-11T07:16:02.400300Z","iopub.status.idle":"2024-09-11T07:16:02.674604Z","shell.execute_reply.started":"2024-09-11T07:16:02.400260Z","shell.execute_reply":"2024-09-11T07:16:02.672517Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Token: i, Tag: O, Meaning: Outside of any entity\nToken: am, Tag: O, Meaning: Outside of any entity\nToken: md, Tag: O, Meaning: Outside of any entity\nToken: sa, Tag: B-PER, Meaning: Beginning of a Person entity\nToken: ##zza, Tag: I-PER, Meaning: Inside a Person entity\nToken: ##t, Tag: I-PER, Meaning: Inside a Person entity\nToken: ho, Tag: I-PER, Meaning: Inside a Person entity\nToken: ##ssa, Tag: I-PER, Meaning: Inside a Person entity\nToken: ##in, Tag: I-PER, Meaning: Inside a Person entity\nToken: ., Tag: I-PER, Meaning: Inside a Person entity\nToken: i, Tag: O, Meaning: Outside of any entity\nToken: love, Tag: O, Meaning: Outside of any entity\nToken: bangladesh, Tag: O, Meaning: Outside of any entity\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, label2idx, num_epochs=5, learning_rate=0.001):\n    loss_function = nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding labels\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n        total_loss = 0\n        \n        for sentences, labels in train_loader:\n            model.zero_grad()\n            tag_scores = model(sentences)\n            \n            # Flatten the output and labels to compute the loss\n            tag_scores = tag_scores.view(-1, len(label2idx))\n            labels = labels.view(-1)\n\n            loss = loss_function(tag_scores, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n\n# Initialize and train the model\nvocab_size = len(word2idx)\ntagset_size = len(label2idx)\n\nmodel = BiLSTMNERModel(vocab_size, tagset_size)\ntrain_model(model, train_loader, val_loader, label2idx)\n","metadata":{},"execution_count":null,"outputs":[]}]}